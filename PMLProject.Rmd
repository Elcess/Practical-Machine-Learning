---
title: "Weight Lifting Exercise"
author: "K. Elcess"
date: "February 25, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("plyr", lib.loc="~/R/win-library/3.3")
library("MASS", lib.loc="~/R/win-library/3.3")
library("ggplot2", lib.loc="~/R/win-library/3.3")
library("ggExtra", lib.loc="~/R/win-library/3.3")
library("AppliedPredictiveModeling", lib.loc="~/R/win-library/3.3")
library("ElemStatLearn", lib.loc="~/R/win-library/3.3")
library("lattice", lib.loc="~/R/win-library/3.3")
library("caret", lib.loc="~/R/win-library/3.3")
library("data.table", lib.loc="~/R/win-library/3.3")
library("DBI", lib.loc="~/R/win-library/3.3")
library("dplyr", lib.loc="~/R/win-library/3.3")
library("pgmm", lib.loc="~/R/win-library/3.3")
library("rpart", lib.loc="~/R/win-library/3.3")
library("e1071", lib.loc="~/R/win-library/3.3")
library("forecast", lib.loc="~/R/win-library/3.3")
library("gbm", lib.loc="~/R/win-library/3.3")
library("randomForest", lib.loc="~/R/win-library/3.3")
library("elasticnet", lib.loc="~/R/win-library/3.3")
```

## Overview

Data related to measurements taken during a weight lifting exercise were obtained from http://groupware.les.inf.puc-rio.br/har. Six subjects, under the supervision of a trainer, performed a unilateral dumbbell curl under each of five conditions. In one condition the exercise was performed exactly as specified; in the other four, a particular error in form was introduced. Four sensors were used to take measurements, one each on the belt, arm, forearm, and dumbbell. Each sensor provided 38 outputs. The goal is to train a model to correctly predict how the exercise was performed from these measurements, *i.e.*, into which of the five classes (correct, error 1, error 2, error 3, or error 4) the particular observation falls.

## Exploratory Analysis

Data was downloaded and the training set read. The training set was then split 60-20-20 into a true training set, a cross-validation set, and a testing set (named "preset" to avoid confusion with the true testing data).

```{r EDA, cache=TRUE}
## Obtain train and test data, and read in train data.
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
        "pml-training.csv")
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", 
        "pml-testing.csv")
nontest <- fread("pml-training.csv")

## Set up train, cross-validation, and pretest sets from train data.
set.seed(123)
tridx <- createDataPartition(nontest$classe, p = 0.6, list = FALSE)
train <- nontest[tridx]
cvtest <- nontest[-tridx]
set.seed(456)
cvidx <- createDataPartition(cvtest$classe, p = 0.5, list = FALSE)
cv <- cvtest[cvidx]
pretest <- cvtest[-cvidx]

## Clean up workspace
rm("cvtest", "nontest")
```

The training data set was cut into "data" and "other" sets for exploratory analysis. Columns in the "data" set incorrectly characterized as containing character data were set to numeric; all other data was left untouched, retaining blank and NA values. The data-containing set was then cut by sensor, and the same 13 measurements from each of the 4 sensors were found to have valid entries for all observations. The measurements were named "roll_sensor,", "pitch_sensor," "yaw_sensor," "total_accel_sensor," "gyros_sensor_x," "gyros_sensor_y," "gyros_sensor_z," "accel_sensor_x," "accel_sensor_y," "accel_sensor_z," "magnet_sensor_x," "magnet_sensor_y," and "magnet_sensor_z," where "sensor" is one of "belt," "arm," "dumbbell," and "forearm." These were interpreted to be the direct output measurements from the gyroscope, the accelerometer, and the magnetometer in the indicated direction, and the total acceleration, roll, pitch, and yaw calculated from the raw measurements.

```{r dataclean, cache=TRUE, results='hide'}
## Change columns misclassified as "character" to "numeric" to enable processing.
leftcols <- select(train, 1:7)
ycol <- select(train, 160)
datacols <- select(train, 8:159)
options(warn = -1)
datacols <- mutate_if(datacols, is.character, as.numeric)

## Break up data by sensor.
belt <- grepl("belt",names(datacols))
beltcols <- select(datacols, which(belt))
arm <- grepl("_arm",names(datacols))
armcols <- select(datacols, which(arm))
dbell <- grepl("bbell",names(datacols))
dbellcols <- select(datacols, which(dbell))
fore <- grepl("fore",names(datacols))
forecols <- select(datacols, which(fore))

## Look into missing data.
beltna <- vector(mode = "integer", length = 38)
armna <- vector(mode = "integer", length = 38)
dbellna <- vector(mode = "integer", length = 38)
forena <- vector(mode = "integer", length = 38)

for(j in 1:38) {
        beltna[j] <- sum(is.na(beltcols[,j]))
        armna[j] <- sum(is.na(armcols[,j]))
        dbellna[j] <- sum(is.na(dbellcols[,j]))
        forena[j] <- sum(is.na(forecols[,j]))
}

## Take only columns that have entries for all observations. These are the same 13
## measurements for each sensor. Put them back together into a predictors data table.
beltcomp <- select(beltcols, which(beltna == 0))
armcomp <- select(armcols, which(armna == 0))
dbellcomp <- select(dbellcols, which(dbellna == 0))
forecomp <- select(forecols, which(forena == 0))
predictors <- cbind(beltcomp, armcomp, dbellcomp, forecomp)

## Create a character vector including the values of ycol for later use.
ychar <- vector(mode = "character", length = 11776)
ychar[(ycol == "A")] <- "A"
ychar[(ycol == "B")] <- "B"
ychar[(ycol == "C")] <- "C"
ychar[(ycol == "D")] <- "D"
ychar[(ycol == "E")] <- "E"
```

An initial model was trained using caret's default method (random forest, "rf") and all 52 of these complete variables as predictors using just the data-containing columns and the outcome column to allow for some increase in speed of computation.

```{r mod1, cache=TRUE}
## Try an all-in model with the train() defaults and explore from there.
df <- cbind(predictors, ycol)
mod1 <- train(classe ~ ., data = df)
mod1import <- mod1$finalModel$importance
mod1rnames <- names(df)
mod1rnames <- mod1rnames[order(mod1import, decreasing = TRUE)]  # top 7 have importance >~ 500
mod1results <- mod1$results  # Accuracy = 0.9862
mod1conf <- mod1$finalModel$confusion  # max class.error = 1.71%
```

The mean decrease in the Gini index ("importance factor") was used to select predictors. After the first seven predictors there was a striking reduction in the importance, so these variables were chosen for the rest of the modeling explorations. They were "roll_belt," "pitch_forearm," "yaw_belt," "magnet_dumbbell_z," "pitch_belt," "roll_forearm," and  "magnet_dumbbell_y."

## Exploratory Modeling

The top 7 predictors from the initial 52-variable model were used for subsequent exploratory model runs using the reduced dataset. Reducing the number of predictors from 52 to 7 for the default random forest model only reduced accuracy from 0.9862 to 0.9768, indicating that the simpler model is sufficient for the purpose. Other model types investigated included k-means clustering, stochastic gradient boosting (gbm), linear discriminant analysis (lda), support vector models (svm) with a linear kernel, and svm with a radial kernel. The random forest model had the highest accuracy.

```{r mod2, cache=TRUE}
## Repeat, using only the top 7 predictors, and check accuracy drop-off.
mod2 <- train(classe ~ roll_belt + pitch_forearm + yaw_belt + magnet_dumbbell_z +
        pitch_belt + roll_forearm + magnet_dumbbell_y, data = df)
mod2results <- mod2$results  # Accuracy = 0.9768
mod2conf <- mod2$finalModel$confusion  # max class.error = 2.46%
mod2import <- mod2$finalModel$importance
mod2rnames <- mod1rnames[1:7]
mod2rnames <- mod2rnames[order(mod2import, decreasing = TRUE)]  # top 6 have importance > 1150
```

```{r mod3, cache=TRUE}
## Try using top 7 from mod1 with "gbm" method.
mod3 <- train(classe ~ roll_belt + pitch_forearm + yaw_belt + magnet_dumbbell_z +
        pitch_belt + roll_forearm + magnet_dumbbell_y, data = df, method="gbm",
        verbose = FALSE)
mod3results <- mod3$results  # Accuracy = 0.9215
```

```{r mod4, cache=TRUE}
## Try using top 7 from mod1 with "lda" method.
mod4 <- train(classe ~ roll_belt + pitch_forearm + yaw_belt + magnet_dumbbell_z +
        pitch_belt + roll_forearm + magnet_dumbbell_y, data = df, method="lda")
mod4results <- mod4$results  # Accuracy = 0.4215 (but fast)
```

```{r km, cache=TRUE}
## Try k means with 5 clusters.
kmod <- kmeans(predictors, centers = 5, nstart = 25)
kclasses <- fitted(kmod, method = "classes")
plot(as.factor(ycol$classe), kclasses, main = "Predictions of a K-means Cluster Model on Train Data",
     xlab = "True ClassE", ylab = "Cluster Predicted by Model")  # Not good
```

```{r mod5, cache=TRUE}
## Try svm using top 7 from mod1
mod5 <- train(classe ~ roll_belt + pitch_forearm + yaw_belt + magnet_dumbbell_z +
        pitch_belt + roll_forearm + magnet_dumbbell_y, data = df, method = "svmLinear2")
mod5results <- mod5$results  # Accuracy = 0.4244
```

```{r mod6, cache=TRUE}
mod6 <- svm(classe ~ roll_belt + pitch_forearm + yaw_belt + magnet_dumbbell_z +
        pitch_belt + roll_forearm + magnet_dumbbell_y, data = df, 
        type = "C-classification")
mod6fit <- (mod6$fitted == ychar)
mod6acc <- sum(mod6fit)/length(mod6fit)  # Accuracy = 0.7762
mod6conf <- matrix(0, nrow = 5, ncol = 5, dimnames = list(c("A","B","C","D","E"),
        c("A","B","C","D","E")))  # Look at confusion matrix
for(i in 1:5) {
        chari <- LETTERS[i]
        for(j in 1:5) {
                charj <- LETTERS[j]
                mod6conf[i,j] <- sum(mod6$fitted == chari & ychar == charj)
        }
}

```

## Model Refinement and Cross-validation

An attempt was made to improve accuracy on model 2, the random forest with 7 predictors, using the cross-validation method in the train function. The accuracy did increase slightly, but so did the maximum misclassification error. When the least important predictor was removed from the model, accuracy remained very high, but the maximum misclassification error increased to unacceptable levels.

```{r refmod, cache=TRUE}
m7trctl <- trainControl(method = "cv")
mod7 <- train(classe ~ roll_belt + pitch_forearm + yaw_belt + magnet_dumbbell_z +
        pitch_belt + roll_forearm + magnet_dumbbell_y, data = df, trControl = m7trctl)
mod7results <- mod7$results  # Accuracy = 0.9839
mod7conf <- mod7$finalModel$confusion  # max class.error = 2.72%

## Try reducing the number of predictors by 1.
mod8 <- train(classe ~ roll_belt + pitch_forearm + yaw_belt + magnet_dumbbell_z +
        pitch_belt + magnet_dumbbell_y, data = df, trControl = m7trctl)
mod8results <- mod8$results  # Accuracy = 0.9777
mod8conf <- mod8$finalModel$confusion  # max class.error = 3.51% <- too high at last
```

Two 7-predictor random forest models, one with training cross-validation and one without, were re-trained against the full train dataset and were tested against the cross-validation dataset. Note that the train data were not manipulated in any way after the data were split.

```{r retrain, cache=TRUE}
md2 <- train(classe ~ roll_belt + pitch_forearm + yaw_belt + magnet_dumbbell_z +
        pitch_belt + roll_forearm + magnet_dumbbell_y, data = train)
md2results <- md2$results  # Accuracy = 0.9763; max class.error = 2.63%
md7 <- train(classe ~ roll_belt + pitch_forearm + yaw_belt + magnet_dumbbell_z +
        pitch_belt + roll_forearm + magnet_dumbbell_y, data = train, trControl = m7trctl)
md7results <- md7$results  # Accuracy = 0.9828; max class.error = 2.68%
```

```{r xv, cache=TRUE}
## Create a character vector including the values of cv$classe for later use.
cvchar <- vector(mode = "character", length = 3923)
cvchar[(cv$classe == "A")] <- "A"
cvchar[(cv$classe == "B")] <- "B"
cvchar[(cv$classe == "C")] <- "C"
cvchar[(cv$classe == "D")] <- "D"
cvchar[(cv$classe == "E")] <- "E"

md2pred <- predict(md2, newdata = cv)
md7pred <- predict(md7, newdata = cv)

md2acc <- sum(md2pred == cvchar)/length(cvchar)  # Accuracy = 0.9834, slightly higher than on train
md7acc <- sum(md7pred == cvchar)/length(cvchar)  # Accuracy = 0.9839, slightly higher than on train

## Compute confusion matrices and classification error rates.
md2conf <- matrix(0, nrow = 5, ncol = 5, dimnames = list(c("A","B","C","D","E"),
        c("A","B","C","D","E")))
for(i in 1:5) {
        chari <- LETTERS[i]
        for(j in 1:5) {
                charj <- LETTERS[j]
                md2conf[i,j] <- sum(md2pred == chari & cvchar == charj)
        }
}
md2ermx <- md2conf
diag(md2ermx) <- 0
md2clerr <- rowSums(md2ermx)/rowSums(md2conf)  # max class.error = 2.77%

md7conf <- matrix(0, nrow = 5, ncol = 5, dimnames = list(c("A","B","C","D","E"),
          c("A","B","C","D","E")))
for(i in 1:5) {
        chari <- LETTERS[i]
        for(j in 1:5) {
                charj <- LETTERS[j]
                md7conf[i,j] <- sum(md7pred == chari & cvchar == charj)
        }
}
md7ermx <- md7conf
diag(md7ermx) <- 0
md7clerr <- rowSums(md7ermx)/rowSums(md7conf)  # max class.error = 2.74%

## Plot predictions vs. data for each model
md2pdf <- data.frame(cbind(md2pred, cvchar))
md7pdf <- data.frame(cbind(md7pred, cvchar))
labels <- unique(cvchar)
levels(md2pdf$md2pred) <- labels
levels(md7pdf$md7pred) <- labels
```

```{r plot, cache=TRUE, echo=FALSE}
g2cv <- ggplot(data = md2pdf, aes(x=md2pred, y=cvchar, color = cvchar)) + geom_count() +
        scale_size_area(name="Predictions") + scale_color_discrete(name="ClassE") +
        labs(x="Predicted ClassE", y="True ClassE", title=
                "Predictions of Model 2 on CV Data")

g7cv <- ggplot(data = md7pdf, aes(x=md7pred, y=cvchar, color = cvchar)) + geom_count() +
        scale_size_area(name="Predictions") + scale_color_discrete(name="ClassE") +
        labs(x="Predicted ClassE", y="True ClassE", title=
                "Predictions of Model 7 on CV Data")

source('~/.R/multiplot.R')
multiplot(g2cv, g7cv, cols = 2)
```

The accuracy for both models is slightly higher than it was for each on the training set, although the maximum classification error is slightly elevated for both. Choose the cv-trained model (Model 7) to proceed with. Test it on the pre-test data. The accuracy should be greater than 95%, perhaps 97%, and the maximum classification error should be less than 3%.

## Testing

Model 7 was run against the test portion of the original training set (named "pretest" to distinguish it from the true test data) using the predict() function.

```{r testing, cache=TRUE}
## Create a character vector including the values of pretest$classe for later use.
ptchar <- vector(mode = "character", length = 3923)
ptchar[(pretest$classe == "A")] <- "A"
ptchar[(pretest$classe == "B")] <- "B"
ptchar[(pretest$classe == "C")] <- "C"
ptchar[(pretest$classe == "D")] <- "D"
ptchar[(pretest$classe == "E")] <- "E"

md7predpt <- predict(md7, newdata = pretest)
md7accpt <- sum(md7predpt == ptchar)/length(ptchar)  # Accuracy = 0.9822, slightly lower than on train & cv

## Generate confusion matrix and calculate classification error
md7confpt <- matrix(0, nrow = 5, ncol = 5, dimnames = list(c("A","B","C","D","E"),
          c("A","B","C","D","E")))
for(i in 1:5) {
        chari <- LETTERS[i]
        for(j in 1:5) {
                charj <- LETTERS[j]
                md7confpt[i,j] <- sum(md7predpt == chari & cvchar == charj)
        }
}
md7ermxpt <- md7confpt
diag(md7ermxpt) <- 0
md7clerrpt <- rowSums(md7ermxpt)/rowSums(md7confpt)  # max class.error = 3.37%

```

## Results and Expectations

Accuracy of 0.9822 on the test data was slightly lower than on the train and cv sets, but above anticipated accuracy. The maximum classification error of 3.37%, though, was greater than expected. Given these results and the small size of the test dataset, I expect an accuracy between 90% and 95% with a maximum classification error between 5% and 10% on the true test data.

## Reference

Data: Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz4Zd5dNY4X

The multiplot() function comes from *Cookbook for R* by Winston Chang.
